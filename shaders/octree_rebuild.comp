/* ---------------------------------------------------------------- *\
 * octree_rebuild.comp
 * Author: Gavin Ralston
 * Date Created: 2025-03-08
 * 
 * NOTE: Don't try to invoke this shader with octree depth = 0.
 * Remember that the model starts at layer 1.
\* ---------------------------------------------------------------- */
#version 460

#define WORKGROUP_SIZE 2

#define PI 3.14

layout (local_size_x = WORKGROUP_SIZE, local_size_y = WORKGROUP_SIZE, local_size_z = WORKGROUP_SIZE) in;

struct OctreeNode
{
	uint indirection;
	uint voxel_type;
};

layout (std430, binding = 0) buffer octree_ssbo
{
	OctreeNode octree[];
};

layout (std430, binding = 1) buffer defrag_accel_tree_ssbo
{
	uint defrag_accel_tree[];
};

layout (std140, binding = 2) readonly uniform num_octree_layers_ubo
{
	uint num_octree_layers;
};

uint getPoolIndex(in uint depth, in uvec3 pos);
uint readIndirectionPool(in uint base_location, in uint node_index);
uint readUniformityPool(in uint base_location, in uint node_index);
uint readVoxelTypePool(in uint base_location, in uint node_index);
void insertFreelistVal(in uint index, in bool val);
uint calculatePoolSize(in uint depth);
uint calculateDefragAccelTreeOffset(in uint depth);
void rebuildDefragAccelTree(in uint depth, in uvec3 pos);

uint octree_depth;
uint octree_width;
int half_octree_width;

// There will be problems with octree_width values approaching UINT_MAX, but i really
// dont think models that large will be rotated anyway.
// If my math is correct the largest octree width that could posibly work is 2^28, but
// even then im pretty sure the octree pool would be too large to be represented with
// 32-bit unsigned ints.
void main()
{
	octree_width = gl_NumWorkGroups.x * gl_WorkGroupSize.x;
	//octree_width = 1u << octree_depth;
	half_octree_width = int(octree_width >> 1);
	//for (octree_depth = 1u; 1u << octree_depth != octree_width; octree_depth++);
	octree_depth = 1u;
	while (1u << octree_depth != octree_width)
	{
		octree_depth++;
	}

	// calculate pos from instance index
	uvec3 pos = uvec3(gl_GlobalInvocationID);

	// calculate the pool index in the input octree
	uint pool_index = getPoolIndex(octree_depth, pos);	

	// loop through all children and check if they are of the same type
	uint child_pool_index_base = getPoolIndex(octree_depth+1, pos*2);
	uint voxel_type = octree[child_pool_index_base].voxel_type;
	uint defrag_accel_tree_index = calculateDefragAccelTreeOffset(octree_depth)
			+ pool_index;
	bool can_merge = true;
	for (uint child = 0; child < 8; child++)
	{
		if (octree[child_pool_index_base+child].voxel_type != voxel_type
		    || octree[child_pool_index_base+child].indirection != 0)
		{
			can_merge = false;
		}
	}
	if (can_merge)
	{
		octree[pool_index].voxel_type = voxel_type;
		octree[pool_index].indirection = 0;
/*
		for (uint child = 0; child < 8; child++)
		{
			insertFreelistVal(child_pool_index_base+child, true);
			//octree[child_pool_index_base+child].voxel_type = 0;
		}
*/
		defrag_accel_tree[defrag_accel_tree_index] = 1;
	}
	else
	{
		octree[pool_index].voxel_type = 0; // TODO: LOD?
		octree[pool_index].indirection = child_pool_index_base >> 3;
		defrag_accel_tree[defrag_accel_tree_index] = 0;
	}
	//insertFreelistVal(pool_index, false);
	
	for (uint tmp_depth = num_octree_layers-1;
	     tmp_depth > octree_depth;
	     tmp_depth--)
	{
		rebuildDefragAccelTree(tmp_depth, pos);
	}
	
	return;
}


uint getPoolIndex(in uint depth, in uvec3 pos)
{
	//uint base_offset = ((1 << (3*(depth-1))) - 1) / 7 * 8;
	uint base_offset = calculatePoolSize(depth-1);
	uint index = 0u;
	//for (int i = 0; i < 10; i++)
	for (int i = 0; i < depth; i++)
	{
		index = bitfieldInsert(index, bitfieldExtract(pos.x, i, 1), i*3, 1);
		index = bitfieldInsert(index, bitfieldExtract(pos.y, i, 1), i*3+1, 1);
		index = bitfieldInsert(index, bitfieldExtract(pos.z, i, 1), i*3+2, 1);
	}
	return base_offset + index;
}


void insertFreelistVal(in uint index, in bool val)
{
/*
	// NOTE: doesn't work because of multithread stuff
	uint bit = (val) ? 1u : 0u;
	uint list_index = index >> 5;
	int bit_offset = 31 - int(index & 31u);
	freelist[list_index] = bitfieldInsert(freelist[list_index], bit, bit_offset, 1);
*/
/*
	uint list_index = index >> 5;
	int bit_offset = 31 - int(index & 31u);
	uint bitfield = bitfieldInsert(0u, 1, bit_offset, 1);
	if (val)
	{
		atomicOr(freelist[list_index], bitfield);
	}
	else
	{
		atomicAnd(freelist[list_index], ~bitfield);
	}
	return;
*/
}


uint calculatePoolSize(in uint depth)
{
	return ((1 << (3*depth)) - 1) / 7 * 8;
}


uint calculateDefragAccelTreeOffset(in uint depth)
{
	//uint previous_layer_pool_size = ((1 << (3*(depth-1))) - 1) / 7 * 8;
/*
	uint previous_layer_pool_size = calculatePoolSize(depth-1);
	return (previous_layer_pool_size*(previous_layer_pool_size+1))/2;
*/
	// TODO: derecursivify this equation
	uint ret = 0u;
	for (uint i = 1; i < depth; i++)
	{
		ret += calculatePoolSize(i);
	}
	return ret;
}


void rebuildDefragAccelTree(in uint depth, in uvec3 pos)
{
	uint pool_index = calculateDefragAccelTreeOffset(depth);
	pool_index += getPoolIndex(octree_depth, pos);	
	//uint child_pool_base_index = calculateDefragAccelTreeOffset(depth+1);
	uint child_pool_base_index = calculateDefragAccelTreeOffset(depth);
	child_pool_base_index += getPoolIndex(octree_depth+1, pos*2);	
	
	uint num_free_indices = 0u;
	for (uint child = 0; child < 8; child++)
	{
		num_free_indices += defrag_accel_tree[child_pool_base_index+child];
	}
	defrag_accel_tree[pool_index] = num_free_indices;

	return;
}
